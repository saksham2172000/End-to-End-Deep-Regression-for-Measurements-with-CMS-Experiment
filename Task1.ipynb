{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.losses import binary_crossentropy\n",
    "from sklearn.metrics import confusion_matrix,recall_score,precision_score,f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_electron=h5py.File(\"ML4SCI_GSoC-main/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5\",\"r\")\n",
    "f_photon=h5py.File(\"ML4SCI_GSoC-main/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5\",\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Splitting the dataset into training, validation and testing and then combining.\n",
    "- Taking 60% as training, 20% validation and 20% as testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split=int((f_electron[\"X\"].shape[0])*0.6)\n",
    "val_split=int((f_electron[\"X\"].shape[0])*0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_photon=f_photon[\"X\"][:train_split]\n",
    "X_val_photon=f_photon[\"X\"][train_split:train_split+val_split]\n",
    "X_test_photon=f_photon[\"X\"][train_split+val_split:]\n",
    "Y_train_photon=f_photon[\"y\"][:train_split]\n",
    "Y_val_photon=f_photon[\"y\"][train_split:train_split+val_split]\n",
    "Y_test_photon=f_photon[\"y\"][train_split+val_split:]\n",
    "\n",
    "X_train_electron=f_electron[\"X\"][:train_split]\n",
    "X_val_electron=f_electron[\"X\"][train_split:train_split+val_split]\n",
    "X_test_electron=f_electron[\"X\"][train_split+val_split:]\n",
    "Y_train_electron=f_electron[\"y\"][:train_split]\n",
    "Y_val_electron=f_electron[\"y\"][train_split:train_split+val_split]\n",
    "Y_test_electron=f_electron[\"y\"][train_split+val_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_X_train=np.concatenate((X_train_photon,X_train_electron),axis=0)\n",
    "combine_X_val=np.concatenate((X_val_photon,X_val_electron),axis=0)\n",
    "combine_X_test=np.concatenate((X_test_photon,X_train_electron),axis=0)\n",
    "\n",
    "combine_Y_train=np.concatenate((Y_train_photon,Y_train_electron),axis=0)\n",
    "combine_Y_val=np.concatenate((Y_val_photon,Y_val_electron),axis=0)\n",
    "combine_Y_test=np.concatenate((Y_test_photon,Y_test_electron),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(combine_X_train)  # shuffling of data so that we get random data.\n",
    "random.shuffle(combine_X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"TRAIN_DATA\",combine_X_train)\n",
    "np.save(\"VAL_DATA\",combine_X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=(batch_size,32,32,2)))\n",
    "model.add(Conv2D(64,kernel_size=(3,3),activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128,(3,3),activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=binary_crossentropy,optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(path,batch=5000):\n",
    "    gen=ImageDataGenerator()\n",
    "    generator=gen.flow_from_directory(path,batch_size=batch,target_size=(32,32),class_mode=\"binary\")\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator=data_generator(\"TRAIN_DATA\",batch=batch_size)\n",
    "val_generator=data_generator(\"VAL_DATA\",batch=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Due To Limited Disk I am Running the dataset for only ten epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EarlyStop=EarlyStopping(monitor=\"val_acc\",patience=1)\n",
    "Modelcheck=ModelCheckpoint(\"best_model.h5\",monitor=\"val_loss\",verbose=0,save_best_only=True,save_weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=model.fit_generator(tarin_generator,steps_per_epoch=60,validation_data=val_generator,\n",
    "                         validation_steps=20,callbacks=[EarlyStop,Modelcheck],epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Accuracy and Loss Epoch Wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss=hist.history['val_loss']\n",
    "train_loss=hist.history['loss']\n",
    "train_acc=hist.history['accuracy']\n",
    "val_acc=hist.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Plot for the loss.\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.title(\"Loss vs Epoch\",fontsize=16)\n",
    "plt.plot(val_loss,c='red',label=\"Validation Loss\")\n",
    "plt.plot(train_loss,c=\"navy\",label=\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\",fontsize=14)\n",
    "plt.ylabel(\"Loss\",fontsize=14)\n",
    "plt.style.use(\"seaborn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Plot for accuracy.\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.title(\"Accuracy vs Epoch\",fontsize=16)\n",
    "plt.plot(val_acc,c='red',label=\"Validation Accuracy\")\n",
    "plt.plot(train_acc,c=\"navy\",label=\"Training Accuracy\")\n",
    "plt.xlabel(\"Epoch\",fontsize=14)\n",
    "plt.ylabel(\"Accuracy\",fontsize=14)\n",
    "plt.style.use(\"seaborn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=np.argmax(model.predict(combine_X_test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix=confusion_matrix(combine_Y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Confusion Matrix\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.heatmap(conf_matrix,annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining How good Was The Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=(np.diag(conf_matrix).sum()/np.sum(conf_matrix))*100   # Accuracy of Model\n",
    "recall=recall_score(combine_Y_test,predictions)*100   # Recall Score of Model\n",
    "precision=precision_score(combine_Y_test,predictions)*100  # Precision Score of Model\n",
    "fscore=f1_score(combine_Y_test,predictions)*100   # F1-Score of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df=pd.DataFrame([accuracy,recall,precision,fscore],index=['Accuracy','Recall Score','Precision Score','F-Score'],columns=['Model Evaluation Metrics (%)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
